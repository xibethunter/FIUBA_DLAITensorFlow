{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TENSORFLOW DEVELOPER PROFESSIONAL CERTIFICATE**\n",
    "# Curso 1: Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n",
    "# Semana 3: Enhancing vision with Convolutional Neural Networks\n",
    "\n",
    "*Esta notebook plasma los apuntes traducidos al español del Curso dicatado por [DeepLearning.AI](https://www.deeplearning.ai/courses/), por lo que puede encontrar errores. Las figuras y ecuaciones se han obtenido/adaptado directamente de las diapositivas utilizadas en el curso. Todo el mérito es de los instructores. Simplemente espero que los apuntes sirvan como material de estudio complementario.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENHANCING VISION WITH CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A conversation with Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Fashion MNIST fue creado por Han Xiao, Kashif Rasul, y Roland Vollgraf. Pienso que es realmente genial que ya seas capaz de implementar una red neuronal para realizar esta tarea de clasificación de moda. \n",
    "\n",
    "Es simplemente increíble que conjuntos de datos grandes como este estén disponibles para los estudiantes para que puedan aprender y hace muy fácil el aprender. En esto caso vimos que con solo unas pocas líneas de código, pudimos construir una DNN que te permitió hacer esta clasificación de ropa, obtuvimos una precisión razonable con ella pero era un algoritmo un poco ingenuo el que usamos. Miramos cada píxel en cada imagen, pero quizás hay formas de hacer lo mejor, quizás mirando las características de lo que hace que un zapato sea un zapato y lo que hace que una cartera sea una cartera. ¿Qué crees tú? Sí. \n",
    "\n",
    "Una de las ideas que hace a estas redes neuronales trabajar mucho mejor es usar redes neuronales convolucionales, donde en vez de ver cada píxel y decir \"Oh, ese pixel tiene un valor de 87, ese tiene un valor de 127\". ¿Es esto un zapato o es una cartera? No lo sé. Pero en cambio puedes ver la imagen y decir \"Oh, veo cordones y una suela\". Probablemente es un zapato o decir, \"Veo un asa y una bolsa rectangular debajo\". Probablemente es una cartera. Las redes convolucionales permitirán a los estudiantes hacer esto. \n",
    "\n",
    "Seguro, lo que es realmente interesante sobre las convoluciones es que parecen complicadas pero realmente son bastante sencillas, ¿okay? Es un filtro que pasas sobre una imagen de la misma forma que si estuvieras realzando los bordes, si alguna vez has hecho procesamiento de imágenes. Puede detectar características dentro de la imagen como has mencionado. Con el mismo paradigma de solo etiquetar los datos, podemos dejar que una red neuronal averigüe por si misma que debe buscar cordones y suelas o asas en carteras y aprender a detectar estas cosas por si misma. ¿Vamos a ver qué impacto tendría sobre Fashion MNIST? En el siguiente video aprenderás redes neuronales convolucionales y las usarás para construir un clasificador de moda mucho mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are convolutions and pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, viste cómo podías crear una red neuronal llamada red neuronal profunda para casar los patrones de un conjunto de imágenes de artículos de moda con etiquetas. En solo unos pocos minutos, fuiste capaz de entrenarla para clasificar con una precisión bastante elevada en el conjunto de entrenamiento, pero un poco menor en el conjunto de prueba. Una de las cosas que habrías visto cuando miraste las imágenes es que hay mucho espacio desperdiciado en cada imagen. Aunque solamente hay 784 píxeles, sería interesante ver si habría alguna manera de condensar la imagen a sus características importantes que distinguen lo que hace que sea un zapato, o una cartera, o una camisa. Ahí es donde entran las convoluciones. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_02.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "¿Qué es una **convolución**? Podrías preguntar. Si alguna vez has hecho algún tipo de procesamiento de imagen, usualmente implica tener un filtro y pasarlo sobre la imagen para cambiar la imagen subyacente. El proceso funciona un poco como esto. Para cada píxel, tomas su valor, y miras los valores de sus vecinos. Si nuestro filtro es 3x3, podemos echar un vistazo al vecino inmediato, tienes la cuadrícula correspondiente de 3x3. Para obtener el nuevo valor para el píxel, simplemente multiplicamos cada vecino por el correspondiente valor en el filtro. Así, por ejemplo: \n",
    "- en este caso nuestro píxel tiene el valor 192, y su vecino superior izquierdo tiene el valor 0. El valor superior izquierdo en el filtro es -1, multiplicamos 0 por -1. \n",
    "- Haríamos lo mismo para el vecino superior. Su valor es 64, el valor del filtro correspondiente era cero, los multiplicaríamos. \n",
    "\n",
    "Repite esto para cada vecino cada valor de filtro correspondiente, y luego tendrías el nuevo píxel con la suma de cada uno de los valores de los vecinos multiplicado por el correspondiente valor del filtro, eso es una convolución. Es tan sencillo como eso. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_03.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "La idea es que algunas convoluciones cambiarán la imagen de tal forma que ciertas características en la imagen se enfaticen. Por ejemplo si ves este filtro las líneas verticales en la imagen sobresaldrán. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_04.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Con este filtro, sobresaldrán las líneas horizontales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es una introducción muy básica a lo que hacen las convoluciones, y cuando se combinan con algo llamado **pooling**, pueden llegar a ser muy potentes. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_05.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Simplemente, el pooling es una forma de comprimir una imagen. Una forma rápida y simple de hacerlo, es ir sobre la imagen cuatro píxeles a la vez, por ejemplo, el píxel actual y sus vecinos de abajo y de su derecha. De estos cuatro, selecciona el mayor valor y guarde precisamente ese. Por ejemplo lo puedes ver aquí. Mis 16 píxeles de la izquierda se convierten en cuatro píxeles de la derecha, viéndolas en cuadrículas de dos por dos y seleccionando el mayor valor. Esto preservará las características que fueron resaltadas por la convolución, a la vez que divide simultáneamente por cuatro el tamaño de la imagen. Tenemos los ejes horizontales y verticales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding convolutions and pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conceptos introducidos en este vídeo están disponibles como capas [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) y capas [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) en TensorFlow. Aprenderás a implementarlos en código en el siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a las convoluciones y al pooling en el código. No tenemos que hacer todas las matemáticas para filtrar y comprimir, solo definimos las capas convolucionales y de pooling para que hagan el trabajo.\n",
    "Este es el código de nuestro ejemplo anterior, donde definimos una red neuronal con una capa de entrada con la forma de nuestros datos, una capa de salida con la forma del número de categorías que estamos intentando definir, y una capa oculta en el medio. `Flatten()` toma nuestra imagen cuadrada de 28x28 y la convierte en un vector de una dimensión.\n",
    "\n",
    "``` python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "```\n",
    "\n",
    "Para añadir convoluciones a esto, se usa código como este. \n",
    "\n",
    "``` python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', \n",
    "                          input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "```\n",
    "Verás que las últimas líneas son las mismas, la capa `Flatten`, la capa `Dense` oculta con 128 neuronas, y la capa de salida `Dense` con 10 neuronas. La diferencia es lo que se ha añadido encima de esto. Echemos un vistazo a esto, línea por línea.\n",
    "\n",
    "Estamos especificando la primera convolución. Estamos pidiendo a Keras que nos genere 64 filtros. Estos filtros son 3x3, su activación es `relu`, es decir los valores negativos se descartarán, finalmente la forma de la entrada es como antes, 28x28. Ese 1 extra solo significa que estamos usando un solo byte para el color. Como vimos antes nuestra imagen es en escala de grises, y solo usamos un byte.\n",
    "\n",
    "Por supuesto, se preguntarán qué son esos 64 filtros. Está un poco fuera del alcance de esta clase definirlos, pero no son aleatorios. Empiezan con un conjunto de filtros bien conocidos en una forma similar al ajuste de patrones que viste anteriormente, los que trabajan a partir de ese conjunto, se aprenden con el tiempo.\n",
    "\n",
    "Para más detalles sobre las convoluciones y cómo funcionan, hay un estupendo conjunto de recursos [aquí](https://bit.ly/2UGa7uH)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn more about convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya has visto cómo añadir una capa convolucional 2D a la parte superior de tu red neuronal en el vídeo anterior. Si quieres ver más detalles sobre su funcionamiento, consulta la lista de reproducción en https://bit.ly/2UGa7uH. Pero ten en cuenta que no es necesario para completar este curso.\n",
    "\n",
    "Ahora vamos a echar un vistazo a la adición de la puesta en común, y el acabado de las convoluciones para que pueda probarlos ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea de código creará la capa de pooling. Es **max-pooling** porque vamos a tomar el valor máximo. Es un pooling 2x2, así por cada cuatro píxeles, el mayor sobrevivirá como se mostró antes. \n",
    "\n",
    "Añadimos otra capa convolucional, y otra capa de max-pooling, para que la red pueda aprender otro conjunto de convoluciones sobre las existentes, y nuevamente hacemos pooling para reducir el tamaño. \n",
    "\n",
    "En el momento que la imagen llega a `Flatten()` para entrar a las capas densas, ya es mucho más pequeña. Ha sido dividida por cuatro, y nuevamente dividida por cuatro. Su contenido ha sido enormemente simplificado, *el objetivo es que las convoluciones van a filtrar las características que determinan la salida*. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_13.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Un método realmente muy útil en este modelo es el método `model.summary()`. Esto te permite inspeccionar las capas del modelo, y ver el trayecto de la imagen a través de las convoluciones, esta es la salida. \n",
    "\n",
    "Es una buena tabla mostrándonos las capas, y algunos de sus detalles incluyendo la forma de la salida. Es importante prestar atención a la columna de la forma de la salida. Cuando lo ves por primera vez, puede ser un poco confuso y sentirlo como un error. Después de todo, ¿no son los datos de 28x28, entonces por qué la salida es de 26x26?. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_15.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "La clave para esto es recordar que el filtro es un filtro 3x3. Considera lo que sucede cuando empiezas a barrer una imagen empezando por la parte superior izquierda. Por ejemplo en esta imagen el perro de la derecha, se puede ver en grande en los píxeles de su esquina superior izquierda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/C1_W3_Página_16.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_17.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/C1_W3_Página_18.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W3_Página_19.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No puedes calcular el filtro para el píxel de la parte superior izquierda, porque no tiene ningún vecino ni encima, ni a su izquierda. De forma similar, el próximo pixel a su derecha no funcionará tampoco porque no tiene vecinos encima de él. Lógicamente, el primer píxel con el que puedes hacer cálculos es este de aquí (Se refiere a los dos de abajo del grupo de imagenes), por que este por supuesto tiene los 8 vecinos que un filtro tres por tres necesita. \n",
    "\n",
    "Esto, cuando lo piensas, significa que no puedes usar un margen de un píxel alrededor de toda la imagen, para que la salida de la convolución sea 2 píxeles más pequeña en x, y 2 píxeles más pequeña en y. Si tu filtro fuera de 5x5 por razones similares tu salida será 4 más pequeña en x y 4 más pequeña en y. \n",
    "\n",
    "Esto es porqué con un filtro 3x3, nuestra salida partiendo de la imagen de 28x28, es ahora de 26x26, hemos quitado 1 píxel en x e y, y en cada uno de los bordes. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/01.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Lo siguiente es la primera de las capas de max-pooling. Recuerda que especificamos que fuera de 2x2, transformando así cuatro píxeles en uno, y teniendo nuestro x e y. Nuestra salida queda reducida de 26x26, a 13x13. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/02.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Las convoluciones van a trabajar sobre eso, y por supuesto perdemos el píxel de margen como antes, llegamos a 11x11, añades otra max-pooling de 2x2 para tenerlo redondeado hacia abajo, y llegar a bajar a imágenes de 5x5. La red neuronal es la misma que antes, pero está alimentada con imágenes de 5x5 en vez de las de 28x28. Pero recuerda, no es solo una imagen comprimida de 5x5 en vez de la original de 28x28, hay un número de convoluciones por imagen que hemos especificado, en este caso 64. Hay 64 nuevas imágenes de 5x5 que han sido suministradas. \n",
    "\n",
    "Hacemos `Flatten()` en eso y tienes 25 píxeles por 64, que son 1600. Puedes ver que la capa después de `Flatten()` tiene 1.600 elementos, contra los 784 que tenía antes. Este número es afectado por los parámetros que configuraste cuando definiste las capas convolucionales 2D. \n",
    "\n",
    "Más tarde cuando experimentes, verás el impacto de configurar otros valores para el número de convoluciones , y particularmente podrás ver qué pasa cuando suministras menos de 748 píxeles. El entrenamiento debería ser más rápido, ¿pero existe algún punto donde es más preciso? Pasemos al cuaderno, para intentarlo nosotros mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting hands-on, your first ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya has visto cómo convertir tu Red Neuronal Profunda en una Red Neuronal Convolucional añadiendo capas convolucionales en la parte superior, y haciendo que la red se entrene contra los resultados de las convoluciones en lugar de los píxeles en bruto. En el siguiente vídeo, verás un libro de trabajo para ver cómo funciona..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Fashion classifier with convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el video anterior, viste las convoluciones y vislumbraste cómo funcionan. Pasando filtros sobre una imagen para reducir la cantidad de información, permitía a la red neuronal extraer eficazmente las características que pueden distinguir una clase de imagen de otra. También viste cómo el pooling comprime la información para hacerla más manejable. Esta es una muy buena forma de mejorar el rendimiento de nuestro reconocimiento de imagen. Echémosle un vistazo en acción usando un cuaderno. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/03.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Esta es la misma red neuronal que usaste antes para cargar el conjunto de imágenes de ropa para luego clasificarlos. Al final del epoch cinco, puedes ver que la pérdida es de alrededor de 0.29, lo que significa, que la precisión es bastante buena con los datos de entrenamiento. Solo llevó unos segundos el entrenamiento, no está mal. Con los datos de prueba como antes y como se esperaba, las pérdidas son un poco más altas y por lo tanto, la precisión es un poco más baja.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/04.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora puedes ver el código que añade convoluciones y poolings. Vamos a crear dos capas convolucionales cada una con 64 convoluciones, y cada una seguida de una capa de max-pooling. Puedes ver que hemos definido las convoluciones de 3x3 y el pooling de 2x2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/05.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Vamos a entrenar. Lo primero que observarás es que el entrenamiento es mucho más lento. Por cada imagen, se tratan 64 convoluciones, y luego la imagen se comprime y luego otras 64 convoluciones, y se comprime nuevamente, y luego se pasa por la DNN, y son 60.000 imágenes que pasan en cada epoch. Podría tomar unos cuantos minutos en vez de unos segundos. \n",
    "\n",
    "Una vez que ha terminado, puedes ver que la pérdida ha mejorado un poco. En este caso, la precisión ha aumentado un poco tanto para los datos de prueba como para los datos de entrenamiento. Eso es muy bueno. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/06.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Echemos un vistazo al código al final del cuaderno. Esta es una visualización muy divertida del trayecto de la imagen a través de las convoluciones. Primero, imprimiré las primeras 100 etiquetas de prueba. El número 9 como vimos antes es un zapato o bota. Escogí algunos ejemplos de esto, la etiqueta 0, la 23 y la 28 son todas 9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/07.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/08.jpg\", style=\"width:50%;\" >\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo al trayecto. El API de Keras nos da cada convolución y cada pooling y cada Dense, etc. como capas. Con el API de capas, puedo ver la salida de cada capa, crearé una lista de cada salida de las capas. Puedo tratar cada artículo de la capa como un modelo de activación individual si solo deseo ver los resultados de esa capa. \n",
    "\n",
    "Pasando a través de las capas, puedo mostrar el trayecto de la imagen a través de la primera convolución luego el primer pooling y luego la segunda convolución y luego el segundo pooling. Observa cómo el tamaño de la imagen cambia mirando los ejes. \n",
    "\n",
    "Si configuro el número de convoluciones a 1, podemos ver que casi inmediatamente detecta el área de los cordones como una característica común entre los zapatos. Por ejemplo, si cambio la tercera imagen a uno, que se parece a una cartera, verás que también tiene una línea clara cerca de la parte inferior que podría parecerse a la suela de los zapatos, pero para el tiempo que ha pasado por las convoluciones, se ha perdido, el área de los cordones ni siquiera aparece. \n",
    "\n",
    "Esta convolución definitivamente me ayuda a distinguir un zapato de una cartera. Nuevamente, si digo que sea un 2, parece que son pantalones, pero la característica que detectó algo que los zapatos tenían en común, falla de nuevo. Asimismo, si cambiase mi tercera imagen de nuevo por esta de zapato, pero pruebo un número diferente de convoluciones, verás que la convolución dos, no encuentra realmente ninguna característica común. Para ver rasgos en común en una imagen diferente, probamos las imágenes 2, 3 y 5. Todas estas parecen ser pantalones. Las convoluciones dos y cuatro parecen detectar esta característica vertical como algo que todas tienen en común. Si vuelvo a la lista encuentro tres etiquetas que son la misma, en este caso 6, puedo ver lo que significan. Si lo ejecuto, puedo ver que parecen ser camisetas. La convolución 4 no hace mucho más, así que probemos 5. Podemos ver cómo el color parece iluminarse en este caso. Intentemos la convolución 1. No sé tú pero yo puedo jugar con esto todo el día. \n",
    "\n",
    "Luego ves lo que hacen cuando lo ejecutas tú mismo. Cuando termines de jugar, trata de cambiar el código con estas recomendaciones, editando las convoluciones, quitando la convolución final, y añadiendo más, etc. Asimismo, en el ejemplo anterior, añadiste una retrollamada que terminaba el entrenamiento cuando la pérdida llegaba a cierto valor. Prueba a añadirla aquí. Cuando termines, pasaremos al siguiente escenario, que es tratar con imágenes más grandes y más complejas que estas. Para ver cómo las convoluciones pueden detectar características cuando no están siempre en el mismo lugar, como estarían en estas imágenes de 28x28 estrechamente controladas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it for yourself (Lab 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Aquí está el cuaderno original](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_1_improving_accuracy_using_convolutions.ipynb) que Laurence estaba usando en ese screencast. Si estás ejecutando esto en Colab, el tiempo de ejecución de la GPU ya ha sido preseleccionado para que el entrenamiento sea más rápido.\n",
    "\n",
    "Trabaja a través de él, y prueba algunos de los ejercicios en la parte inferior. Merece la pena dedicar un poco de tiempo a estos ejercicios porque, como antes, te ayudarán a ver el impacto de pequeños cambios en varios parámetros del código. Deberías dedicar al menos una hora a esto hoy.\n",
    "\n",
    "Una vez que hayas terminado, pasa al siguiente vídeo y echa un vistazo al código para construir una convolución tú mismo para visualizar cómo funciona.\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_1_improving_accuracy_using_convolutions_traduccion.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking through convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las lecciones previas, viste el impacto que las convoluciones y el pooling tienen sobre la eficiencia y el aprendizaje de tus redes, pero mucho de eso era teórico en naturaleza. Pensé que sería interesante hackear unos códigos juntos para demostrar cómo funciona una convolución en realidad. También crearemos un algoritmo de pooling, para que puedas visualizar su impacto. Hay un cuaderno con el que puedes jugar también, voy a ir paso a paso a través de ello. \n",
    "\n",
    "Este es el cuaderno para jugar con convoluciones. Usa unas pocas librerías de Python con las que puedes no estar familiarizado como **cv2**. También tiene Matplotlib que usamos antes. Si no las has usado, son realmente bastante intuitivas para esta tarea son muy fáciles de aprender. Primero, configuraremos las entradas y en particular, importaremos la librería misc de **SciPy**. Este es un buen atajo para nosotros porque `misc.ascent` devuelve una buena imagen con la que podemos jugar, y no tenemos que preocuparnos de manejarla nosotros mismos. **Matplotlib** contiene el código para dibujar una imagen y mostrarla bien en el navegador con Colab. Podemos ver la imagen ascent de SciPy. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/10.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "A continuación, tomaremos una copia de la imagen, y la añadiremos a nuestras convoluciones caseras, y crearemos variables para hacer un seguimiento de las dimensiones x e y de la imagen. Podemos ver que es una imagen de 512x512. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/11.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Vamos a crear una convolución como una matriz de 3x3. La llenaremos con valores que son muy buenos para detectar bordes nítidos primero. Aquí es donde crearemos la convolución. Iteramos sobre la imagen, dejando un píxel de margen. Verás que el bucle empieza en uno y no en cero, y termina en un tamaño x-1 y tamaño y-1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/12.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "El bucle, calculará el valor de la convolución viendo el píxel y sus vecinos, y luego multiplicándolos por los valores determinados por el filtro, antes de finalmente sumarlo todo. Vamos a ejecutarlo. Solo lleva unos pocos segundos, cuando termina, vamos a dibujar los resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/13.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Podemos ver que solo algunas características pasan a través del filtro. He proporcionado un par más de filtros, vamos a probarlos. Este primero es realmente bueno detectando líneas verticales. Cuando lo ejecuto y dibujo los resultados, podemos ver que las líneas verticales de la imagen pasaron a su través. Es realmente genial porque no solo son de arriba a abajo, son verticales en perspectiva dentro de la perspectiva de la propia imagen. \n",
    "\n",
    "Similarmente, este filtro funciona muy bien para líneas horizontales. Cuando la ejecuto, y luego dibujo los resultados, podemos ver que muchas de las líneas horizontales pasaron el filtro. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/14.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Echemos un vistazo al pooling, y en este caso max-pooling, que toma píxeles en grupos de cuatro y solo pasa el de mayor valor. Ejecuto el código y luego dibujo la salida. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/15.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Podemos ver que las características de la imagen se mantienen. pero mira los ejes de cerca, y podemos ver que el tamaño se ha reducido a la mitad de 500 a 250. Por diversión, podemos probar el otro filtro ejecutarlos y luego comparar la convolución con la versión con pooling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/16.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "\n",
    "Nuevamente, podemos ver que las características no solo se han mantenido, sino que también se han enfatizado. Así es cómo funcionan las convoluciones. De forma no visible, TensorFlow prueba diferentes filtros en tu imagen y aprende cuáles funcionan mientras mira los datos de entrenamiento. Como resultado, cuando funciona, obtendrás una gran reducción de la información que pasa a través de la red, porque aísla e identifica las características, puedes también obtener un incremento de la precisión. Juega con los filtros en este cuaderno mira si puedes lograr algunos efectos interesantes por ti mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with filters and pools (Lab 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[En esta Notebook origianl](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_2_exploring_convolutions.ipynb) para probar algunas convoluciones. Si se te ocurre algún filtro interesante!.\n",
    "\n",
    "Como antes, dedica un poco de tiempo a jugar con este cuaderno. Prueba diferentes filtros, e investiga diferentes tipos de filtros. Hay algo de información divertida sobre ellos [aqui](https://lodev.org/cgtutor/filtering.html).\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W3/ungraded_labs/C1_W3_Lab_2_exploring_convolutions_traduccion.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 1 </b></font>\n",
    "\n",
    "¿Cómo mejoran las convoluciones el reconocimiento de imágenes?\n",
    "\n",
    "- Aceleran el procesamiento de las imágenes\n",
    "- Aíslan las características de las imágenes\n",
    "- Hacen que la imagen sea más pequeña\n",
    "- Hacen que la imagen sea más clara\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 2 </b></font>\n",
    "\n",
    "¿Qué hace la técnica del Pooling a las imágenes?\n",
    "\n",
    "- Reduce la información en ellas manteniendo algunas características\n",
    "- Las hace más nítidas\n",
    "- Las combina\n",
    "- Aísla las características de las imágenes\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 3 </b></font>\n",
    "\n",
    "Verdadero o falso. Si pasas una imagen de 28x28 por un filtro de 3x3 la salida será de 26x26.\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 4 </b></font>\n",
    "\n",
    "Verdadero o falso. Después de hacer un max pool de una imagen de 26x26 con un filtro de 2x2, la salida será de 56x56\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 5 </b></font>\n",
    "\n",
    "¿Cómo afecta al entrenamiento el uso de Convoluciones en nuestra red neuronal profunda?\n",
    "\n",
    "- Su impacto dependerá de otros factores.\n",
    "- Lo hace más rápido\n",
    "- No afecta al entrenamiento\n",
    "- Lo hace más lento\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta 1: Aíslan las características de las imágenes.\n",
    "    \n",
    "Respuesta 2: Reduce la información en ellas manteniendo algunas características.\n",
    "  \n",
    "Respuesta 3: Verdadero.\n",
    "    \n",
    "Respuesta 4: Falso.\n",
    "    \n",
    "Respuesta 5: Su impacto dependerá de otros factores.\n",
    "    \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEKLY ASSIGNMENT - IMPROVING DNN PERFORMANCE USING CONVOLUTIONS\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W3/assignment/C1W3_Assignment_traduccion.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
