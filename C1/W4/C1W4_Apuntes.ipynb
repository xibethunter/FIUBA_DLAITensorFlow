{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TENSORFLOW DEVELOPER PROFESSIONAL CERTIFICATE**\n",
    "# Curso 1: Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\n",
    "# Semana 4: Using Real-world Images\n",
    "\n",
    "*Esta notebook plasma los apuntes traducidos al español del Curso dicatado por [DeepLearning.AI](https://www.deeplearning.ai/courses/), por lo que puede encontrar errores. Las figuras y ecuaciones se han obtenido/adaptado directamente de las diapositivas utilizadas en el curso. Todo el mérito es de los instructores. Simplemente espero que los apuntes sirvan como material de estudio complementario.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING REAL-WORLD IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A conversation with Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que el clasificador que construimos para clasificación de moda, usando convoluciones pudimos hacerlo más eficiente y más preciso. Pensaba que era realmente genial pero aún es muy limitado en el escenario porque todas nuestras imágenes son 28 por 28 y el objeto está de hecho centrado. Y aunque es un conjunto de datos fabuloso para aprender, cuando empezamos a adquirir imágenes del mundo real e imágenes complejas posiblemente necesitemos ir un poco más lejos. ¿Qué crees? \n",
    "\n",
    "Creo que es genial que tomar la idea central de una red convolucional te permita implementar un algoritmo que pueda encontrar no solo bolsos que están el centro de la imagen sino en cualquier lugar de la imagen, puede ser llevado por alguien en la izquierda o en la derecha de una imagen mucho mayor, una imagen de 1 millón de píxeles. Esto es 1000 por 1000 píxeles. Asimismo, para muchas aplicaciones en vez de usar escala de grises, se quiere usar imágenes de color. Las mismas ideas centrales pero con un conjunto de datos mayor, imágenes mayores con etiquetas similares te permiten hacerlo. Muy bien. La técnica que estás aprendiendo, te está realmente ayudando a ser capaz de tener éxito en estos escenarios más reales. \n",
    "\n",
    "Sé que estás trabajando en un conjunto de datos de caballos y humanos. Ha sido muy divertido. He estado trabajando en un conjunto de datos que son varias imágenes de caballos moviéndose por la imagen y están en diferentes posiciones, y humanos de la misma forma y humanos variados, hombres, mujeres razas diferentes, ese tipo de cosas para ver si podemos construir un clasificador binario entre ellos dos. Pero lo que es realmente interesante de esto es que todas son imágenes generadas por computadora, pero las podemos usar para clasificar fotos reales. Me he divertido con eso. Creo que será un ejercicio divertido para ti trabajar en ello también. Y si te has preguntado si esos algoritmos que estás aprendiendo son algo real, los algoritmos que estás aprendiendo son realmente algo real que se usan hoy en muchas aplicaciones comerciales. Por ejemplo, si te fijas en la forma en que un vehículo autónomo usa cámaras para detectar otros vehículos o peatones para tratar de evitarlos, usan redes neuronales convolucionales para esa parte de la tarea, muy similar a lo que estás aprendiendo. De hecho, en otros contextos, te he escuchado hablar de usar una red neuronal convolucional. Para tomar una foto, por ejemplo. Podemos tomar una foto de un cultivo e intentar distinguir si tiene una enfermedad en desarrollo. Ha sido genial. Muchas gracias. Ha sido muy divertido. \n",
    "\n",
    "En el siguiente vídeo, aprenderás cómo aplicar las redes neuronales convolucionales a esas imágenes más grandes y más complejas. Por favor continúa con el siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore an impactful, real-world solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentaron Andrew y Laurence, las técnicas que has aprendido ya pueden aplicarse a imágenes complejas, y puedes empezar a resolver escenarios reales con ellas. Hablaron de cómo se puede utilizar, por ejemplo, en la detección de enfermedades con la planta de mandioca, y puedes ver un vídeo que lo demuestra [aquí](https://www.youtube.com/watch?v=NlpS-DhayQA). Una vez que lo hayas visto, ¡pasa a la siguiente lección!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este punto, has construido un clasificador de imagen que funcionaba usando una red neuronal profunda y viste cómo mejorar su rendimiento añadiendo convoluciones. Una limitación, sin embargo, era que usaba un conjunto de datos de imágenes muy uniforme. Imágenes de ropa que fueron puestas y enmarcadas en 28 por 28. ¿Pero qué pasa si se usan imágenes más grandes donde las características pueden estar en diferentes posiciones? \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W4_Página_02.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "¿Por ejemplo, en estas imágenes de caballos y humanos? Tienen diferentes tamaños y diferentes proporciones de aspecto. El sujeto puede estar en diferentes posiciones. En algunos casos, puede haber hasta múltiples sujetos. Además, los ejemplos anteriores con datos de moda usaban un conjunto de datos incorporado. Todos los datos estaban divididos fácilmente en conjuntos de entrenamiento y conjuntos de prueba y había etiquetas disponibles. En muchos escenarios ese no será el caso y tendrás que hacerlo tú mismo. En esta lección, echaremos un vistazo a algunos las API disponibles para hacértelo más fácil. En particular, el **generador de imágenes de TensorFlow**. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W4_Página_03.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Una característica del generador de imágenes es que puedes dirigirla a un directorio, luego sus subdirectorios generarán automáticamente las etiquetas para ti. Por ejemplo, considera esta estructura de directorios. Tienes un directorio de imágenes y en él, tienes subdirectorios para entrenamiento y validación. Cuando pones subdirectorios en ellos para caballos y humanos y almacenas las imágenes requeridas ahí, el generador de imágenes puede crear un alimentador para esas imágenes y las etiqueta automáticamente para ti. Por ejemplo, si dirijo un generador de imágenes al directorio de entrenamiento, las etiquetas serán caballos y humanos y todas las imágenes en cada directorio serán cargadas y etiquetadas como corresponde. \n",
    "Similarmente, si dirijo uno al directorio de validación pasará lo mismo. \n",
    "\n",
    "Echemos un vistazo a esto en el código. \n",
    "\n",
    "``` python\n",
    "from tensorflow.keras.preprocessing.image\n",
    "import ImageDataGenerator\n",
    "```\n",
    "\n",
    "La clase generador de imágenes está disponible en `keras.preprocessing.image`. \n",
    "\n",
    "Puedes entonces instanciar un generador de imágenes así. Pasamos `rescale` por él para normalizar los datos. \n",
    "\n",
    "``` python\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "Puedes luego llamar al método `flow_from_directory` para que cargue las imágenes de ese directorio y sus subdirectorios. \n",
    "\n",
    "``` python\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=128,\n",
    "    class_mode='binary')\n",
    "```\n",
    "\n",
    "Es un error común que la gente dirija el generador al subdirectorio. Fallará si se hace así. Debes siempre apuntar al *directorio* que contiene los subdirectorios que contienen las imágenes. El nombre de los subdirectorios serán las etiquetas de las imágenes que están contenidas en ellos. Asegúrate de que el directorio al que te diriges es el correcto. Lo pones como el segundo parámetro como `train_dir`. Las imágenes podrán venir en todo tipo de formas y tamaños y desafortunadamente para entrenar una red neuronal, los datos de entrada tienen que ser todos del mismo tamaño, las imágenes deberán ser redimensionada para hacerlas consistentes. \n",
    "\n",
    "Lo bueno de este código es que las imágenes son redimensionadas por ti cuando son cargadas. No necesitas preprocesar miles de imágenes en tu sistema de archivos. Pero podrías haberlo hecho si lo deseas. La ventaja de hacerlo en el momento de la ejecución, es que puedes experimentar con tamaños diferentes sin afectar los datos originales. Aunque el conjunto de datos de humanos y caballos ya es de 300 por 300, cuando uses otros conjuntos de datos pueden no estar siempre uniformemente dimensionados. Esto es realmente útil para ti. \n",
    "\n",
    "Las imágenes serán cargadas para el entrenamiento y validación en lotes (`batch_size`) lo que es más eficiente que hacerlo una por una. Hay toda una ciencia para calcular el tamaño del lote que va más allá del alcance de este curso, pero puedes experimentar con diferentes tamaños para ver el impacto en el rendimiento cambiando este parámetro. \n",
    "\n",
    "Finalmente, está `class_mode`. Este es un clasificador binario, por ejemplo, elige entre dos cosas distintas; caballos y humanos, lo especificamos aquí. Otras opciones en particular para más de dos cosas se explorarán más adelante en el curso. \n",
    "\n",
    "El generador de validación debe ser exactamente lo mismo excepto, por supuesto, que se dirige a un directorio diferente, el que contiene los subdirectorios que contienen las imágenes de prueba. \n",
    "\n",
    "``` python\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=128,\n",
    "    class_mode='binary')\n",
    "```\n",
    "\n",
    "Cuando veas los cuadernos dentro de poco, verás cómo descargar las imágenes como un zip, y luego organizarlos en subdirectorios de entrenamiento y prueba, y luego poner subdirectorios de caballos y humanos en cada uno. Eso es Python puro. No es TensorFlow ni cualquier otro asunto de aprendizaje profundo. Todo esto está explicado en el cuaderno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has visto cómo un `ImageDataGenerator` puede hacer fluir imágenes desde un directorio y realizar operaciones como redimensionarlas sobre la marcha, lo siguiente es diseñar la red neuronal para manejar estas imágenes más complejas. Lo verás en el siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a ConvNet to use complex images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la definición de red neuronal que usaremos para clasificar caballos versus humanos. Es bastante similar a lo que acabas de usar para los artículos de moda, pero hay unas pocas pequeñas diferencias basadas en estos datos, y el hecho de que estamos usando generadores. Este es el código. \n",
    "\n",
    "``` python\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', \n",
    "                          input_shape=(300,300,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "Como puedes ver está `sequential` como antes con convoluciones y poolings antes de llegar a las capas dense al final. Pero destaquemos algunas de las diferencias. \n",
    "- Primero que nada, observa que hay tres conjuntos de convolución y pooling arriba. Esto refleja la mayor complejidad y tamaño de las imágenes. Recuerda de antes, el 28 por 28, bien, ahora tenemos 300 por 300. Empezamos por 298 por 298 y luego la mitad, etc. hasta que al final estamos en una imagen de 35 por 35. Podemos incluso añadir otro par de capas si deseamos llegar al mismo tamaño de antes, pero lo mantendremos en 3 por ahora.\n",
    "- Otra cosa a la que prestar atención es la forma de la entrada (`input_shape`). Redimensionamos la imagen para que sea de 300 por 300 cuando sean cargadas, pero también son imágenes de color. Son tres bytes por píxel. Un byte para el rojo, un byte para el verde, y un byte para el canal azul, este es un patrón común de color de 24-bit. \n",
    "- Si estás prestando mucha atención, puedes ver que la capa de salida también ha cambiado. Recuerda que antes cuando creaste la capa de salida, tenías una neurona por clase, pero ahora solo hay una neurona para dos clases. Eso es porque estamos usando una función de activación diferente que es sigmoid que es genial para la clasificación binaria, cuando una clase tiende a cero, la otra clase tiende a uno. Podrías usar dos neuronas si lo deseas, y la misma función softmax como antes, pero para binary esto es un poco más eficiente. \n",
    "\n",
    "Si lo deseas puedes experimentar con el cuaderno e intentarlo tú mismo. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/C1_W4_Página_18.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Si echamos un vistazo al resumen de nuestro modelo, podemos ver el trayecto de los datos de la imagen a través de las convoluciones. Los 300 por 300 se convierten en 298 por 298 después del filtro de tres por tres, se lleva a 149 por 149 que a su vez se reduce a 73 por 73 después del filtro que luego es llevada a 35 por 35, esto luego pasa por Flatten(), 64 convoluciones que son 35 al cuadrado y esa forma será alimentada a la DNN. Si multiplicas 35 por 35 por 64, obtienes 78400 y esa es la forma de los datos una vez que sale de las convoluciones. Si solo hubiéramos suministrado imágenes sin tratar de 300 por 300 sin las convoluciones, eso sería más de 900000 valores. Ya lo hemos reducido bastante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the ConvNet with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has diseñado la red neuronal para clasificar Caballos o Humanos, el siguiente paso es entrenarla a partir de los datos que están en el sistema de archivos, que pueden ser leídos por los generadores. Verás cómo hacerlo en el siguiente vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora compilaremos el modelo y como siempre tenemos una función de pérdida y un optimizador. Cuando clasificábamos diez artículos de moda, recordarás que la función de pérdida era `categorical_cross_entropy`. Pero como estamos haciendo una elección binaria, escogemos una `binary_cross_entropy` en cambio. Asimismo, antes usábamos un optimizador `Adam`. Podrías hacer lo mismo nuevamente, pero creo que sería divertido usar `RMSprop`, donde puedes ajustar la tasa de aprendizaje para experimentar con el rendimiento. \n",
    "\n",
    "``` python\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=RMSprop(lr=0.001),\n",
    "             metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Para entender la tasa de aprendizaje y cómo todo encaja, revisa este genial [vídeo](http://youtu.be/zLRB4oupj6g) de deeplearning.ai que entra en más detalle. Por ahora, no voy a entrar en los detalles en este curso. \n",
    "\n",
    "Lo siguiente es el entrenamiento, esto parece un poco diferente al de antes cuando llamábamos a `model.fit`. Porque ahora llamas a `model.fit_generator`, eso es porque estamos usando un generador en vez de un conjunto de datos. Recuerda el generador de imágenes anterior, veamos cada parámetro en detalle. \n",
    "\n",
    "\n",
    "``` python\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    step_per_epoch=8,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=8\n",
    "    verbose=2)\n",
    "```\n",
    "\n",
    "El primer parámetro es `train_generator` que configuraste anteriormente. Esto hace fluir las imágenes desde el directorio de entrenamiento. Recuerda el tamaño del lote que usaste cuando lo creaste, era 20, eso es importante en el siguiente paso. Hay 1.024 imágenes en el directorio de entrenamiento, estamos cargándolas de 128 en 128 cada vez. \n",
    "\n",
    "Para cargarlas todas tenemos que hacerlo en 8 lotes. Configuramos `step_per_epoch` para lograrlo.\n",
    "\n",
    "Aquí configuramos el número de `epochs` para entrenar. Esto es un poco más complejo, digamos, 15 epochs en este caso. \n",
    "\n",
    "Ahora especificamos el conjunto de validación que viene del `validation_generator` que creamos anteriormente. Tenía 256 imágenes y queríamos tratarlas en lotes de 32, haremos 8 pasos (`validation_steps`).\n",
    "\n",
    "El parámetro `verbose` especifica cuánto se muestra en el entrenamiento está teniendo lugar. Con verbose puesto a 2, obtenemos menos animación ocultando el progreso del epoch. \n",
    "\n",
    "Una vez que el modelo esté entrenado, querrás, por supuesto, hacer alguna predicción con el modelo. Este es el código para ello, veámoslo por partes.\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    \n",
    "    # predicting images\n",
    "    path = '/content/' + fn\n",
    "    img = image.load_img(path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    images = np.vstrack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    print(classes[0])\n",
    "    if classes[0]>0.5:\n",
    "        print(fn + \" is a human\")\n",
    "    else:\n",
    "        print(fn + \" is a horse\")\n",
    "```\n",
    "\n",
    "`from google.colab import files`, `uploaded = files.upload()` y `uploaded.keys()` son partes específicas para Colab, que son las que te facilitan el botón que puedes presionar para seleccionar una o más imágenes para cargar. La ruta de la imagen se carga en esta lista llamada uploaded. \n",
    "\n",
    "El bucle itera a través de todas las imágenes de esta colección. Puedes cargar una imagen y prepararla para meterla en el modelo con este código. Toma nota para asegurar que las dimensiones coinciden con las dimensiones de la entrada que especificaste cuando diseñaste el modelo. \n",
    "\n",
    "Puedes llamar a `model.predict`, y pasarle los detalles, devolverá un vector de `classes`. En el caso de clasificación binaria, este solo contendrá un artículo con un valor cercano a 0 para una clase y cercano a 1 para la otra.\n",
    "\n",
    "Más tarde en este curso verás la clasificación multi clase con softmax. Donde obtendrás una lista de valores con un valor de probabilidad de cada clase, todas las probabilidades suman 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ha pasado por el código para definir la red neuronal, entrenarla con imágenes en el disco, y luego predecir valores para nuevas imágenes. Vamos a ver esto en acción en un libro de trabajo. En el siguiente vídeo, Laurence te guiará a través del código, y después, podrás probarlo por ti mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking through developing a ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/01.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "Acabas de ver cómo empezar a crear una red neuronal con Keras que usa el generador de imágenes para cargar y etiquetar automáticamente los archivos basado en los subdirectorios. Veamos cómo podemos usarlo para construir un clasificador de caballos o humanos con una red neuronal convolucional. Este es el primer cuaderno que puedes intentar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/02.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "Para empezar, descargarás el archivo zip que contiene los datos de caballos y humanos. Una vez que eso está hecho, puedes descomprimirlo en un directorio temporal en esta máquina virtual. El archivo zip contiene dos carpetas; uno llamado caballos filtrados, y otro llamado humanos filtrados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/03.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "Cuando se descomprime, se crearon por ti. Vamos a dirigir un par de variables a ellos, y luego podemos explorar los archivos imprimiendo algunos nombres de archivos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/04.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "Esto se puede usar para generar etiquetas, pero no vamos a necesitarlo si usamos el generador de Keras. Si deseas usar estos datos sin él, los nombres de los archivos serán sus etiquetas, sin embargo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/05.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Imprimiremos las imágenes con las que tenemos que trabajar, y hay un poco más de 1000, ahora podemos mostrar algunas imágenes aleatoriamente del conjunto de datos. Podemos ver ocho caballos y ocho humanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/06.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Un aspecto interesante de este conjunto de datos es que todas las imágenes son generadas por computadora. Las he reproducido para que sean lo más parecidas posible a una foto, se van a usar de hecho para clasificar fotos reales de caballos y personas, aquí hay unas cuantas imágenes más solo para mostrar algo de su diversidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/07.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Empecemos a construir el modelo. Primero, importaremos TensorFlow, ahora construiremos las capas. Tenemos bastantes convoluciones aquí porque nuestra fuente de imágenes es bastante grande, son de 300 por 300. Más tarde podemos explorar el impacto de reducir su tamaño y necesitar menos convoluciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/08.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Podemos imprimir el resumen de las capas, y aquí podemos ver el momento que alcanzamos la red densa, las convoluciones bajan hasta siete por siete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/10.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Seguidamente compilaremos la red. Usa `binary_crossentropy` como pérdida, binaria porque usamos solamente dos clases, y el optimizador es RMSprop que nos permite cambiar la tasa de aprendizaje. No te preocupes si no entiendes esto completamente ahora, existen enlaces a los contenidos sobre esto donde puedes aprender más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que has aprendido a descargar y procesar el conjunto de datos de caballos y humanos, estás listo para entrenar. Cuando definiste el modelo, viste que estabas usando una nueva función de pérdida llamada '[Binary Crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/)', y un nuevo [optimizador](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) llamado [RMSProp](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop). Si quieres aprender más sobre el tipo de clasificación binaria que estamos haciendo aquí, ¡mira [este](https://www.youtube.com/watch?v=eqEc66RFY0I&t=6s) gran video de Andrew!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walking through training the ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/11.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Lo siguiente es usar ImageDataGenerator. Lo instanciamos y escalamos las imágenes con 1 / 255, que normaliza los valores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/12.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Lo dirigimos al directorio principal donde vemos los archivos descomprimidos.\n",
    "Podemos ver que encuentra todas las imágenes, que ha asignado a dos clases, porque fueron asignadas a dos subdirectorios. Entrenamos la red neuronal durante 15 epochs, lo que llevará unos dos minutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/13.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Cada epoch carga los datos, calculando las convoluciones y luego tratando de hacer coincidir las convoluciones con las etiquetas. Como puedes ver, la exactitud crece casi siempre, pero alguna vez no, muestra el descenso de gradiente del aprendizaje en acción.\n",
    "\n",
    "Siempre es buena idea vigilar las fluctuaciones en esta figura. Si son muy fuertes, puedes ajustar la tasa de aprendizaje.\n",
    "\n",
    "Recuerda el parámetro `RMSprop` cuando compilaste el modelo, allí es donde lo modificarías. También va bastante rápido porque, estoy entrenando en una máquina GPU.\n",
    "\n",
    "Cuando llegamos a la epoch 15, podemos ver que la exactitud es de 0.9981, lo que realmente es muy bueno. Pero recuerda, esto solamente se basa en los datos que la red ya ha visto durante el entrenamiento, que son solo unas 1000 imágenes. No te dejes llevar por una falsa sensación de seguridad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"left\", src=\"./imagenes/14.jpg\", style=\"width:49%;\" >\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/15.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Divirtámonos ahora un poco con el modelo, veamos si podemos predecir la clase de nuevas imágenes que no se han visto previamente. Vamos a Pixabay y veamos qué podemos encontrar. Buscaré caballos, hay muchos caballos, escojamos este. Es un caballo blanco corriendo en la nieve. La descargaré en mi sistema de archivos. Regresaré ahora al cuaderno, voy a subir la imagen desde mi sistema de archivos. Veremos que ha sido subida y clasificada como un caballo. Vamos a probar otra. Como esta de aquí. La subiré al cuaderno, veremos que también es clasificada como un caballo.\n",
    "\n",
    "Volveré ahora a Pixabay y buscaré personas, seleccionaré esta imagen de una chica sentada en un banco. La descargaré en mi sistema de archivos, la subiré a la red neuronal, y podemos ver que esta es también clasificada correctamente como un humano.\n",
    "\n",
    "Otro ejemplo. Volveré a la lista de resultados de Pixabay, y seleccionaré esta imagen de una chica. Como antes, la descargaré en mi sistema de archivos y la subiré a la red neuronal veremos que también se detecta un humano en la imagen.\n",
    "\n",
    "Otra cosa que puedo hacer con este código es subir archivos múltiples hacer que los clasifique todos a la vez. Y aquí podemos ver todas las clasificaciones. Tenemos cuatro de cuatro correctas.\n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/16.jpg\", style=\"width:50%;\" >\n",
    "</figure>\n",
    "\n",
    "Este cuaderno también incluye algunas visualizaciones de la imagen según pasa a través de las convoluciones. Puedes intentarlo con este código.\n",
    "\n",
    "Aquí puedes ver una imagen de humano que fue convolucionada, características como las piernas sobresalen realmente. Y si lo ejecuto nuevamente, podemos ver otro humano con características similares. También el cabello es bastante distintivo. Juega con esto tú mismo para ver qué descubres.\n",
    "\n",
    "Allí vimos una red neuronal convolucional crear un clasificador de caballos o humanos usando un conjunto de cerca de 1000 imágenes. Las cuatro imágenes que probamos, todas funcionaron, pero eso no es realmente escalable. En el siguiente vídeo, veremos cómo añadir un conjunto de validación al de entrenamiento y también cómo medir automáticamente la exactitud del conjunto de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with the horse or human classifier (Lab 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora te toca a ti. Puedes encontrar el cuaderno [aquí](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation.ipynb). Trabaja en él y hazte una idea de cómo el ImageGenerator extrae las imágenes del sistema de archivos y las introduce en la red neuronal para su entrenamiento. Diviértete un poco con el código de visualización en la parte inferior.\n",
    "\n",
    "En cuadernos anteriores, has ajustado parámetros como las épocas, o el número de capas ocultas y neuronas en ellas. Pruébalo por ti mismo, y mira el impacto que tiene. Dedica algo de tiempo a esto.\n",
    "\n",
    "Una vez que hayas terminado, pasa al siguiente vídeo, donde podrás validar tu entrenamiento con un montón de imágenes.\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation_traducido.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding automatic validation to test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el video anterior, viste cómo construir una red neuronal convolucional que clasificaba caballos contra humanos. Cuando terminaste, hiciste luego unas cuantas pruebas usando imágenes que descargaste de la web. En este vídeo, verás cómo puedes construir validación en el bucle de entrenamiento especificando un conjunto de imágenes de validación, para dejar que TensorFlow haga el trabajo pesado de medir la efectividad con ese conjunto. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/17.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "\n",
    "Como antes, descargamos el conjunto de datos, pero ahora también descargaremos el conjunto de datos de validación por separado. Lo descomprimiremos en dos carpetas separadas, una para entrenamiento, otra para validación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/18.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Crearemos unas variables que apunten a nuestros subdirectorios de entrenamiento y validación, y podemos comprobar los nombres de los archivos. Recuerda que los nombres de los archivos pueden no ser siempre fiables para etiquetas. Por ejemplo, aquí las etiquetas de validación de caballos no están nombradas como tal, mientras que las de humanos sí. Podemos también hacer una comprobación rápida de si tenemos todos los datos, parece que sí, luego podemos proceder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/19.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Podemos mostrar algunas de las imágenes de entrenamiento como hicimos anteriormente, vayamos directamente al modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/20.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Aquí podemos importar TensorFlow, y aquí definimos las capas del modelo. Es exactamente lo mismo que la última vez. Imprimiremos el resumen del modelo, puedes ver que no ha cambiado tampoco. Luego compilamos el modelo con los mismos parámetros que antes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/21.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora, es cuando podemos hacer algunos cambios. Como el generador de imágenes para los datos de entrenamiento, creamos ahora un segundo para los datos de validación. Es casi el mismo flujo. Creamos un generador de validación como una instancia de un generador de imágenes, hacemos rescale para normalizar, y luego lo dirigimos al directorio de validación. Cuando lo ejecutamos, vemos que toma las imágenes y las clases de ese directorio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/22.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora vamos a entrenar la red. Observa los parámetros extras para dejarle saber sobre los datos de validación. Al final de cada epoch a la vez que informa de la pérdida y exactitud en el entrenamiento, también comprueba el conjunto de validación para darnos la pérdida y exactitud aquí. A medida que progresan las epochs deberías ver cómo aumenta constantemente la exactitud de validación siendo ligeramente menor que la de entrenamiento. Solo debería llevar otros dos minutos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/23.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora que hemos llegado al epoch 15, podemos ver que nuestra exactitud es cercana al 97 por ciento en los datos de entrenamiento, y cerca del 85 por ciento en el conjunto de validación, esto es lo que se esperaba. El conjunto de validación consiste en datos que la red neuronal no ha visto previamente, así que es de esperar que rinda un poco peor en él. Pero probemos algunas imágenes más empezando con este caballo blanco.\n",
    "\n",
    "Podemos ver que fue clasificado mal como humano. Probemos este realmente bonito.\n",
    "\n",
    "Podemos ver que es clasificado correctamente como caballo. Probemos algunas personas.\n",
    "\n",
    "Probemos con esta mujer de la blusa azul. Esta es realmente una imagen interesante porque tiene la espalda vuelta, y las piernas están oscurecidas por el vestido, pero ha sido clasificada correctamente como humana. Esta es difícil. Para nosotros ella es humana, ¿pero las alas confundirán a la red neuronal?\n",
    "\n",
    "Efectivamente, ella es tomada por un caballo. Es comprensible particularmente porque el conjunto de entrenamiento tiene muchos caballos blancos contra un fondo herbáceo.\n",
    "\n",
    "¿Qué tal este? Tiene ambos, un caballo y un humano,\n",
    "\n",
    "pero es clasificada como caballo. Podemos ver que la característica dominante en la imagen es el caballo, así que realmente no es sorprendente. También hay muchos caballos blancos en el conjunto de entrenamiento, podría coincidir con ellos. La última. No podía resistirme a esta imagen ya que es adorable,\n",
    "\n",
    "afortunadamente es clasificada como un caballo. Ahora vimos el conjunto de entrenamiento con uno de validación, y pudimos obtener una buena estimación de la exactitud del clasificador mirando los resultados con el conjunto de validación. Usar estos resultados y entender dónde y por qué fallan algunas inferencias, te puede ayudar a entender cómo modificar los datos de entrenamiento para prevenir errores así. Pero cambiemos de marcha en el siguiente vídeo. Echaremos un vistazo al impacto de compactar los datos para hacer el entrenamiento más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get hands-on and use validation (Lab 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora puedes probarlo por ti mismo. [Aquí](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation.ipynb) está el cuaderno que Laurence revisó en el vídeo. Juega con él para ver cómo se entrena y prueba tú mismo algunas imágenes. Una vez que hayas terminado, pasa al siguiente vídeo en el que compactarás tus datos para ver el impacto en el entrenamiento.\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_2_image_generator_with_validation_traducido.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the impact of compressing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las imágenes en los conjuntos de caballos y de humanos son todas de 300 por 300 píxeles. Pusimos unas cuantas capas convolucionales para reducir las imágenes a características condensadas. Esto, por supuesto, puede retardar el entrenamiento. Echemos un vistazo a lo que sucedería si cambiamos a 150 por 150 para las imágenes para tener una cuarta parte de los datos totales y ver cuál sería su impacto. Comenzaremos como antes descargando y descomprimiendo los conjuntos de entrenamiento y de prueba. Luego dirigiremos algunas variables en el conjunto de entrenamiento y de prueba antes de configurar el modelo. \n",
    "\n",
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/24.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Primero, importamos TensorFlow y definimos las capas del modelo. Observa que hemos cambiado la forma de entrada para que sea 150 por 150, y hemos quitado la cuarta y quinta combinación de convolución y max-pooling. El resumen del modelo muestra ahora la capa empezando con 148 por 148, ese fue el resultado de la convolución de 150 por 150. Vamos a ver al final, terminamos con 17 por 17 cuando hemos hecho todas las convoluciones y poolings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/25.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Compilaremos el modelo como antes, y crearemos los generadores como antes, pero ten en cuenta que el tamaño objetivo ha cambiado ahora a 150 por 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img align=\"right\", src=\"./imagenes/26.jpg\", style=\"width:60%;\" >\n",
    "</figure>\n",
    "\n",
    "Ahora podemos comenzar el entrenamiento, y podemos ver que después del primer epoch que el entrenamiento es rápido, y que la exactitud y la validación no son malos tampoco. El entrenamiento continúa y ambos valores de exactitud suben.\n",
    "\n",
    "A menudo, verás valores de exactitud que son realmente altos como 1.000, lo que es probablemente una señal de que estás sobre ajustando. Llegamos al final, tengo realmente una exactitud grande en los datos de prueba, cerca de 0,99, que es demasiado alto. El conjunto de validación es de 0.84, que está bastante bien, pero pongámoslo a prueba con unas imágenes reales.\n",
    "\n",
    "Vamos a empezar con esta imagen de la chica y el caballo. Todavía es clasificada como un caballo. A continuación, vamos a echar un vistazo a este caballo estupendo,que todavía es clasificado correctamente.\n",
    "\n",
    "Estas monadas son también correctamente clasificadas, pero éste todavía es clasificado erróneamente. Pero lo más interesante creo que es esta mujer. Cuando utilizamos 300 por 300 antes y más convoluciones, fue clasificada correctamente. Pero ahora, no lo ha sido. Este es un gran ejemplo de la importancia de medir los datos de entrenamiento contra un conjunto grande de validación, inspeccionando dónde va mal para ver lo que se puede hacer para solucionarlo. Usar este conjunto más reducido es mucho más barato para entrenar, pero entonces habrá errores como el de esta mujer con la espalda vuelta y sus piernas oscurecidas por el vestido, porque no tenemos esos datos en el conjunto de entrenamiento. Ese es un buen consejo sobre cómo editar el conjunto de datos para el mejor efecto en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Hands-on with compacted images (Lab 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pruebe esta versión](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images.ipynb) del cuaderno en la que Laurence ha compactado las imágenes. Puedes ver que los tiempos de entrenamiento mejoran, ¡pero que algunas clasificaciones pueden ser erróneas! Experimenta con diferentes tamaños -- ¡no tienes que usar 150x150 por ejemplo!\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images_traducido.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 1 </b></font>\n",
    "\n",
    "Utilizando el Generador de Imágenes, ¿cómo se etiquetan las imágenes?\n",
    "\n",
    "- Se basa en el directorio que contiene la imagen\n",
    "- TensorFlow lo deduce del contenido\n",
    "- Se basa en el nombre del archivo\n",
    "- Hay que hacerlo manualmente\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 2 </b></font>\n",
    "\n",
    "¿Qué método del Generador de Imágenes se utiliza para normalizar la imagen?\n",
    "\n",
    "- normalizar_imagen\n",
    "- reescalar\n",
    "- reescalar_imagen\n",
    "- normalizar\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 3 </b></font>\n",
    "\n",
    "¿Cómo especificamos el tamaño de entrenamiento de las imágenes?\n",
    "\n",
    "- El parámetro training_size en el generador de validación\n",
    "- El parámetro training_size en el generador de entrenamiento\n",
    "- El parámetro target_size en el generador de validación\n",
    "- El parámetro target_size en el generador de entrenamiento\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 4 </b></font>\n",
    "\n",
    "Cuando especificamos que la forma de entrada es (300, 300, 3), ¿qué significa?\n",
    "\n",
    "- Habrá 300 imágenes, cada una de tamaño 300, cargadas en lotes de 3\n",
    "- Habrá 300 caballos y 300 humanos, cargados en lotes de 3\n",
    "- Cada imagen será de 300x300 píxeles, y deberá haber 3 capas convolucionales\n",
    "- Cada Imagen será de 300x300 píxeles, con 3 bytes para definir el color\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 5 </b></font>\n",
    "\n",
    "Si tus datos de entrenamiento se acercan a 1.000 de precisión, pero tus datos de validación no, ¿cuál es el riesgo aquí?\n",
    "\n",
    "- Está infraajustando los datos de validación\n",
    "- Está sobreajustando los datos de validación\n",
    "- Estás sobreajustando tus datos de entrenamiento\n",
    "- No hay riesgo, es un gran resultado\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 6 </b></font>\n",
    "\n",
    "Las redes neuronales convolucionales son mejores para clasificar imágenes como caballos y humanos porque:\n",
    "\n",
    "- Hay una gran variedad de caballos\n",
    "- En estas imágenes, las características pueden estar en diferentes partes del cuadro\n",
    "- Hay una gran variedad de humanos\n",
    "\n",
    "<font size=\"2\" color=\"red\"><b> PREGUNTA 6 </b></font>\n",
    "\n",
    "Tras reducir el tamaño de las imágenes, los resultados del entrenamiento fueron diferentes. ¿Por qué?\n",
    "\n",
    "- Había menos información en las imágenes\n",
    "- Hemos eliminado algunas convoluciones para manejar las imágenes más pequeñas\n",
    "- El entrenamiento fue más rápido\n",
    "- Había más información condensada en las imágenes\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<details>\n",
    "   <summary><font size=\"2\" color=\"darkblue\"><b> Click para la respuesta</b></font></summary>\n",
    "\n",
    "Respuesta 1: Se basa en el directorio que contiene la imagen.\n",
    "    \n",
    "Respuesta 2: reescalar.\n",
    "  \n",
    "Respuesta 3: El parámetro target_size en el generador de entrenamiento.\n",
    "    \n",
    "Respuesta 4: Cada Imagen será de 300x300 píxeles, con 3 bytes para definir el color.\n",
    "    \n",
    "Respuesta 5: Estás sobreajustando tus datos de entrenamiento.\n",
    "    \n",
    "Respuesta 6: Todas.\n",
    "    \n",
    "Respuesta 7: Hemos eliminado algunas convoluciones para manejar las imágenes más pequeñas.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEEKLY ASSIGNMENT - HANDLING COMPLEX IMAGES\n",
    "\n",
    "> El Laboratorio en español se pueden encontrar\n",
    "> [aqui](https://github.com/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W4/assignment/C1W4_Assignment_traducido.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
