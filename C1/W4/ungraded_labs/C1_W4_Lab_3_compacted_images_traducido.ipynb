{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/IngCarlaPezzone/tensorflow-1-public/blob/main/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images_traducido.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qR8Am0lBtRAx"
   },
   "source": [
    "# Laboratorio no calificado: Efecto de las imágenes compactadas en el entrenamiento\n",
    "\n",
    "En este cuaderno, verá cómo la reducción del tamaño objetivo de las imágenes del generador afectará a la arquitectura y al rendimiento de su modelo. Esta es una técnica útil en caso de que necesite acelerar su entrenamiento o ahorrar recursos de computación. Comencemos.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "# Ungraded Lab: Effect of Compacted Images in Training\n",
    "\n",
    "In this notebook, you will see how reducing the target size of the generator images will affect the architecture and performance of your model. This is a useful technique in case you need to speed up your training or save compute resources. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1iD7DhP2NWt"
   },
   "source": [
    "**NOTA IMPORTANTE:** Este cuaderno está diseñado para ejecutarse como Colab. Ejecutarlo en su máquina local puede dar lugar a que algunos de los bloques de código arrojen errores.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instale este paquete para utilizar la GPU de Colab para el entrenamiento\n",
    "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxY7KvGQ2Qdr"
   },
   "source": [
    "Como antes, comience a descargar los conjuntos de entrenamiento y validación:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "As before, start downloading the train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXZT2UsyIVe_"
   },
   "outputs": [],
   "source": [
    "# Descargar el conjunto de entrenamiento\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mLij6qde6Ox"
   },
   "outputs": [],
   "source": [
    "# Descargar el conjunto de validación\n",
    "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9brUxyTpYZHy"
   },
   "source": [
    "Entonces descomprímelos:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Then unzip them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLy3pthUS0D2"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Descomprimir el conjunto de entrenamiento\n",
    "local_zip = './horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./horse-or-human')\n",
    "\n",
    "# Descomprimir conjunto de validación\n",
    "local_zip = './validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./validation-horse-or-human')\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-qUPyfO7Qr8"
   },
   "source": [
    "A continuación, defina los directorios que contienen las imágenes:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "Then define the directories containing the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_M9nWN-K8B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directorio con fotos de caballos de entrenamiento\n",
    "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
    "\n",
    "# Directorio con fotos de humanos en entrenamiento\n",
    "train_human_dir = os.path.join('./horse-or-human/humans')\n",
    "\n",
    "# Directorio con imágenes de caballos de validación\n",
    "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
    "\n",
    "# Directorio con fotos de humanos de validación\n",
    "validation_human_dir = os.path.join('./validation-horse-or-human/humans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1wrZCxTPw4m"
   },
   "source": [
    "Puede comprobar que los directorios no están vacíos y que el conjunto de trenes tiene más imágenes que el conjunto de validación:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You can check that the directories are not empty and that the train set has more images than the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_horse_names = os.listdir(train_horse_dir)\n",
    "print(f'TRAIN SET HORSES: {train_horse_names[:10]}')\n",
    "\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(f'TRAIN SET HUMANS: {train_human_names[:10]}')\n",
    "\n",
    "validation_horse_hames = os.listdir(validation_horse_dir)\n",
    "print(f'VAL SET HORSES: {validation_horse_hames[:10]}')\n",
    "\n",
    "validation_human_names = os.listdir(validation_human_dir)\n",
    "print(f'VAL SET HUMANS: {validation_human_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTpdVrBg2LZC"
   },
   "outputs": [],
   "source": [
    "print(f'total training horse images: {len(os.listdir(train_horse_dir))}')\n",
    "print(f'total training human images: {len(os.listdir(train_human_dir))}')\n",
    "print(f'total validation horse images: {len(os.listdir(validation_horse_dir))}')\n",
    "print(f'total validation human images: {len(os.listdir(validation_human_dir))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oqBkNBJmtUv"
   },
   "source": [
    "## Construir el modelo\n",
    "\n",
    "El modelo seguirá la misma arquitectura que antes pero la diferencia clave está en el parámetro `input_shape` de la primera capa `Conv2D`. Dado que más tarde se compactarán las imágenes en el generador, es necesario especificar aquí el tamaño de imagen esperado. Así que en lugar de 300x300 como en los dos laboratorios anteriores, se especifica una matriz más pequeña de 150x150.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "## Build the Model\n",
    "\n",
    "The model will follow the same architecture as before but they key difference is in the `input_shape` parameter of the first `Conv2D` layer. Since you will be compacting the images later in the generator, you need to specify the expected image size here. So instead of 300x300 as in the previous two labs, you specify a smaller 150x150 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PixZ2s5QbYQ3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Obsérvese que la forma de entrada es el tamaño deseado de la imagen 150x150 con 3 bytes de color\n",
    "    # Esta es la primera convolución\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # La segunda convolución\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # La tercer convolución\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     # La cuarta convolución (Puedes descomentar la 4ª y 5ª capa conv para ver el efecto)\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "#     # La quinta convolución\n",
    "#     tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Aplanar los resultados para alimentar una DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Capa oculta de 512 neuronas\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Sólo 1 neurona de salida. Contendrá un valor de 0-1 \n",
    "    # donde 0 para una clase ('caballos') y 1 para la otra ('humanos')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9EaFDP5srBa"
   },
   "source": [
    "Se puede ver la diferencia con los modelos anteriores cuando se imprime el `model.summary()`. Como es de esperar, habrá menos entradas a la capa `Dense` al final del modelo en comparación con los laboratorios anteriores. Esto se debe a que ha utilizado el mismo número de capas de agrupación máxima en su modelo. Y como tiene una imagen más pequeña para empezar (150 x 150), entonces la salida después de todas las capas de agrupación también será más pequeña.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You can see the difference from previous models when you print the `model.summary()`. As expected, there will be less inputs to the `Dense` layer at the end of the model compared to the previous labs. This is because you used the same number of max pooling layers in your model. And since you have a smaller image to begin with (150 x 150), then the output after all the pooling layers will also be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKj8392nbgP"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEkKSpZlvJXA"
   },
   "source": [
    "Utilizará los mismos ajustes para el entrenamiento:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "You will use the same settings for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DHWhFP_uhq3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn9m9D3UimHM"
   },
   "source": [
    "### Preprocesamiento de datos\n",
    "\n",
    "Ahora instanciaremos los generadores de datos. Como se mencionó anteriormente, se compactará la imagen especificando el parámetro `target_size`. Vea el sencillo cambio de abajo:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Now you will instantiate the data generators. As mentioned before, you will be compacting the image by specifying the `target_size` parameter. See the simple change below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClebU9NJg99G"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#  Todas las imágenes serán reescaladas por 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Flujo de imágenes de entrenamiento en lotes de 128 utilizando el generador train_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  #  Este es el directorio de origen para las imágenes de entrenamiento\n",
    "        target_size=(150, 150),  # Todas las imágenes serán redimensionadas a 150x150\n",
    "        batch_size=128,\n",
    "        # Como se ha utilizado la pérdida binaria_crossentropy, se necesitan etiquetas binarias\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flujo de imágenes de entrenamiento en lotes de 128 utilizando el generador train_datagen\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # Este es el directorio de origen para las imágenes de entrenamiento\n",
    "        target_size=(150, 150),  # Todas las imágenes se redimensionarán a 150x150\n",
    "        batch_size=32,\n",
    "        # Como se ha utilizado la pérdida binaria_crossentropy, se necesitan etiquetas binarias\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu3Jdwkjwax4"
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Ahora está listo para entrenar y ver los resultados. Observa la rapidez con la que se entrena el modelo y las precisiones que obtienes en los conjuntos de entrenamiento y validación.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "### Training\n",
    "\n",
    "Now you're ready to train and see the results. Note your observations about how fast the model trains and the accuracies you're getting in the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb1_lgobv81m"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=8,  \n",
    "      epochs=15,\n",
    "      verbose=1,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6vSHzPR2ghH"
   },
   "source": [
    "### Predicción del modelo\n",
    "\n",
    "Como es habitual, también es una buena práctica intentar ejecutar su modelo sobre algunas imágenes seleccionadas. Compruebe si ha obtenido un rendimiento mejor, peor o igual que en el laboratorio anterior.\n",
    "\n",
    "**Nota importante:** Debido a algunos problemas de compatibilidad, el siguiente bloque de código dará lugar a un error después de seleccionar las imágenes a cargar si está ejecutando este cuaderno como un `Colab` en el navegador `Safari`. Para todos los demás navegadores, continúe con el siguiente bloque de código e ignore el que le sigue.\n",
    "\n",
    "Para los usuarios de Safari: por favor, comente u omita el bloque de código de abajo, descomente el siguiente bloque de código y ejecútelo.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "### Model Prediction\n",
    "\n",
    "As usual, it is also good practice to try running your model over some handpicked images. See if you got better, worse, or the same performance as the previous lab.\n",
    "\n",
    "**Important Note:** Due to some compatibility issues, the following code block will result in an error after you select the images(s) to upload if you are running this notebook as a `Colab` on the `Safari` browser. For all other browsers, continue with the next code block and ignore the next one after it.\n",
    "\n",
    "_For Safari users: please comment out or skip the code block below, uncomment the next code block and run it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWp43WxJDNT"
   },
   "outputs": [],
   "source": [
    "## BLOQUE DE CÓDIGO PARA NAVEGADORES QUE NO SEAN SAFARI\n",
    "## USUARIOS DE SAFARI: POR FAVOR, OMITA ESTE BLOQUE Y EJECUTE EL SIGUIENTE EN SU LUGAR\n",
    "\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "    # predicting images\n",
    "    path = '/content/' + fn\n",
    "    img = image.load_img(path, target_size=(150, 150))\n",
    "    x = image.img_to_array(img)\n",
    "    x /= 255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    print(classes[0])\n",
    "    if classes[0]>0.5:\n",
    "    print(fn + \" is a human\")\n",
    "    else:\n",
    "    print(fn + \" is a horse\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckps9Sw4657d"
   },
   "source": [
    "Los usuarios de `Safari` tendrán que cargar las imágenes manualmente en su espacio de trabajo. Por favor, siga las instrucciones, descomente el bloque de código de abajo y ejecútelo.\n",
    "\n",
    "Instrucciones para subir imágenes manualmente en un Colab:\n",
    "\n",
    "1. Seleccione el icono de la `carpeta` en la `barra de menú` de la izquierda.\n",
    "2. Haga clic en la `carpeta con una flecha apuntando hacia arriba` llamada `..`.\n",
    "3. Haga clic en la `carpeta` llamada `tmp`.\n",
    "4. Dentro de la carpeta `tmp`, `crea una nueva carpeta` llamada `imágenes`. Verás la opción `Nueva carpeta` haciendo clic en el botón de menú de los `3 puntos verticales` junto a la carpeta `tmp`.\n",
    "5. Dentro de la nueva carpeta `imágenes`, sube una(s) imagen(es) de tu elección, preferiblemente de un caballo o de un humano. Arrastra y suelta la(s) imagen(es) encima de la carpeta `images`.\n",
    "6. Descomente y ejecute el bloque de código que aparece a continuación. \n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "`Safari` users will need to upload the images(s) manually in their workspace. Please follow the instructions, uncomment the code block below and run it.\n",
    "\n",
    "Instructions on how to upload image(s) manually in a Colab:\n",
    "\n",
    "1. Select the `folder` icon on the left `menu bar`.\n",
    "2. Click on the `folder with an arrow pointing upwards` named `..`\n",
    "3. Click on the `folder` named `tmp`.\n",
    "4. Inside of the `tmp` folder, `create a new folder` called `images`. You'll see the `New folder` option by clicking the `3 vertical dots` menu button next to the `tmp` folder.\n",
    "5. Inside of the new `images` folder, upload an image(s) of your choice, preferably of either a horse or a human. Drag and drop the images(s) on top of the `images` folder.\n",
    "6. Uncomment and run the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_GgQjRT65oM"
   },
   "outputs": [],
   "source": [
    "# # CODE BLOCK FOR SAFARI USERS\n",
    "\n",
    "# import numpy as np\n",
    "# from keras.preprocessing import image\n",
    "# import os\n",
    "\n",
    "# images = os.listdir(\"/tmp/images\")\n",
    "\n",
    "# print(images)\n",
    "\n",
    "# for i in images:\n",
    "#  print()\n",
    "#  # predicting images\n",
    "#  path = '/tmp/images/' + i\n",
    "#  img = image.load_img(path, target_size=(150, 150))\n",
    "#  x = image.img_to_array(img)\n",
    "#  x /= 255\n",
    "#  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "#  images = np.vstack([x])\n",
    "#  classes = model.predict(images, batch_size=10)\n",
    "#  print(classes[0])\n",
    "#  if classes[0]>0.5:\n",
    "#    print(i + \" is a human\")\n",
    "#  else:\n",
    "#    print(i + \" is a horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8EHQyWGDvWz"
   },
   "source": [
    "### Visualización de las representaciones intermedias\n",
    "\n",
    "También puede observar de nuevo las representaciones intermedias. Notará que la salida en la última capa de convolución es aún más abstracta porque contiene menos píxeles que antes.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "### Visualizing Intermediate Representations\n",
    "\n",
    "You can also look again at the intermediate representations. You will notice that the output at the last convolution layer is even more abstract because it contains fewer pixels than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5tES8rXFjux"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Definir un nuevo modelo que tomará una imagen como entrada, y emitirá\n",
    "# representaciones intermedias para todas las capas del modelo anterior después de\n",
    "# la primera.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "\n",
    "# Prepara una imagen de entrada aleatoria del conjunto de entrenamiento.\n",
    "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "img_path = random.choice(horse_img_files + human_img_files)\n",
    "img = load_img(img_path, target_size=(150, 150))  # esta es una imagen PIL\n",
    "x = img_to_array(img)  # matriz Numpy con forma (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Matriz Numpy con forma (1, 150, 150, 3)\n",
    "\n",
    "# Escala por  1/255\n",
    "x /= 255\n",
    "\n",
    "# Pasar la imagen por la red, obteniendo así todas\n",
    "# representaciones intermedias de esta imagen.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# Estos son los nombres de las capas, por lo que puede tenerlos como parte de la trama\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Visualizar las representaciones\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    if len(feature_map.shape) == 4:\n",
    "\n",
    "    # Sólo se hace esto para las capas conv / maxpool, no para las capas totalmente conectadas\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "    # El mapa de características tiene forma (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "\n",
    "    # Mosaico de las imágenes en esta matriz\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "        x = feature_map[0, :, :, i]\n",
    "        x -= x.mean()\n",
    "        x /= x.std()\n",
    "        x *= 64\n",
    "        x += 128\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "\n",
    "        # Colocar cada filtro en esta gran cuadrícula horizontal\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "\n",
    "    # Mostrar la grilla\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4IBgYCYooGD"
   },
   "source": [
    "## Limpiar\n",
    "\n",
    "Por favor, ejecute la siguiente celda para terminar el kernel y liberar recursos de memoria:\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "Please run the following cell to terminate the kernel and free memory resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "651IgjLyo-Jx"
   },
   "outputs": [],
   "source": [
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFnBvcIrXWW2"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "En este laboratorio, has visto cómo la compactación de imágenes afectaba a tu modelo anterior. Esta es una técnica a tener en cuenta especialmente cuando aún estás en la fase de exploración de tus propios proyectos. Puedes ver si un modelo más pequeño se comporta igual de bien que un modelo grande y así tener un entrenamiento más rápido. También has visto lo fácil que es personalizar tus imágenes para este ajuste de tamaño simplemente cambiando un parámetro en la clase `ImageDataGenerator`.\n",
    "\n",
    "<details><summary><font size=\"2\" color=\"darkblue\"><b> Texto Original </b></font></summary>\n",
    "\n",
    "## Wrap Up\n",
    "\n",
    "In this lab, you saw how compacting images affected your previous model. This is one technique to keep in mind especially when you are still in the exploratory phase of your own projects. You can see if a smaller model behaves just as well as a large model so you can have faster training. You also saw how easy it is to customize your images for this adjustment in size by simply changing a parameter in the `ImageDataGenerator` class."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W4_Lab_3_compacted_images.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/adding_C1/C1/W4/ungraded_labs/C1_W4_Lab_3_compacted_images.ipynb",
     "timestamp": 1639112249467
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
